{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "# scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluation\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, ExtraTreesRegressor                   \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from xgboost import XGBClassifier\n",
    "#import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from vecstack import stacking\n",
    "\n",
    "\n",
    "# bayesian optimisation\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "import csv\n",
    "from timeit import default_timer as timer\n",
    "from hyperopt import STATUS_OK, hp, tpe, Trials, fmin\n",
    "import ast\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "data_train = pd.read_csv(\"../input/train.csv\", encoding = 'utf-8-sig')\n",
    "data_test = pd.read_csv(\"../input/test.csv\", encoding = 'utf-8-sig')\n",
    "data_sample = pd.read_csv(\"../input/sample_submission.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4388cd2f7a10c7d4b861e06ff92439bc71e38a3"
   },
   "source": [
    "## Check Data\n",
    "- 1. data shape\n",
    "- 2. columns\n",
    "- 3. missing value\n",
    "- 4. check data type -> model cannot deal with type except float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5849eb9af6224c1527b1c5df61b9ffadc698eda5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [1] data shape\n",
    "\n",
    "print(\"Train data shape:\", data_train.shape)\n",
    "print(\"Test data shape:\", data_test.shape)\n",
    "\n",
    "# [2] check columns: train data get 1 column more than the test data\n",
    "print(\"check columns in train data but not in test data: \\n\",\n",
    "      data_train.columns[~data_train.columns.isin(data_test.columns)])\n",
    "print(\"check columns in test data but not in train data: \\n\",\n",
    "      data_test.columns[~data_test.columns.isin(data_train.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb65e888476ced353402d3b659a20333b675e237",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [3] check missing value\n",
    "data_check_missing_val = pd.DataFrame(data_train.isnull().sum()/data_train.isnull().count()).rename(columns={0:\"percentage of missing value\"}).reset_index().rename(columns={\"index\":\"feature_name\"})\n",
    "print(\"feature with missing value: \", data_check_missing_val[data_check_missing_val[\"percentage of missing value\"]!=0].feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3e2395917a9872f1aaab7512a635e3c161f03df",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [4] check column types -> check data type before scaling\n",
    "print(\"train data columns that are not float type: \",\n",
    "      [col for col in data_train.columns if data_train[col].dtype!=float])\n",
    "\n",
    "print(\"test data columns that are not float type: \",\n",
    "      [col for col in data_test.columns if data_test[col].dtype!=float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9735f39b456dd0f397ef5cf4813a53bae5dae0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"---> train data information <--- \\n\")\n",
    "print(data_train.info())\n",
    "print(\"---> test data information <--- \\n\")\n",
    "print(data_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a18cd50c3de9a0d061cd16c317381c78c682a2f1"
   },
   "source": [
    "## Explore Data\n",
    "\n",
    "- 1. summary statistics -> standard deviation & mean too high\n",
    "- 2. check imbalance data\n",
    "- 3. check feature correlation\n",
    "- 4. check duplicate within a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4736eeb6650f89f9609abf7b35ea2d959b9a40d3",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [1]\n",
    "plt.title(\"train data: distribution of standard deviation\")\n",
    "sns.distplot(data_train.describe().loc['std'])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"train data: distribution of mean\")\n",
    "sns.distplot(data_train.describe().loc['mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "213697c22f025481a56131c15d08d41af9501b11",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"test data: distribution of standard deviation\")\n",
    "sns.distplot(data_test.describe().loc['std'])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"test data: distribution of mean\")\n",
    "sns.distplot(data_test.describe().loc['mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec5c46be0c2c3102b978def8eee678f710998751",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [2] check imbalance data\n",
    "print(\"--> % of target data in the data <-- \\n\", \n",
    "      data_train[\"target\"].sum()/len(data_train))\n",
    "\n",
    "sns.countplot(data_train['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ce4c87b3bba3bb415bd3d787bc165075486612d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [3] check feature correlation\n",
    "features = [col for col in data_train.columns if col not in [\"ID_code\", \"target\"]]\n",
    "correlations = data_train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "correlations = correlations[correlations['level_0'] != correlations['level_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2c3a3dce7958e711fa44366f70528f737f9ec24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations[:10]\n",
    "correlations[:-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba8a5a25679a929853211f9aaaef63ed562f05d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [4] check duplicate\n",
    "val_of_col_train = []\n",
    "num_of_dup_train = []\n",
    "val_of_col_test = []\n",
    "num_of_dup_test = []\n",
    "\n",
    "for col in features:\n",
    "    val_of_col_train.append(data_train[col].value_counts().nlargest(1).index.values.tolist())\n",
    "    num_of_dup_train.append(data_train[col].value_counts().nlargest(1).values.tolist())\n",
    "    val_of_col_test.append(data_test[col].value_counts().nlargest(1).index.values.tolist())\n",
    "    num_of_dup_test.append(data_test[col].value_counts().nlargest(1).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcb32bdd50fcaa5ab21556a03b2663a8efb80cf3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_check_duplicates_train = pd.concat([pd.DataFrame(features),\n",
    "                                   pd.DataFrame(val_of_col_train),\n",
    "                                   pd.DataFrame(num_of_dup_train)], axis=1)\n",
    "col_list = [\"col_name\", \"most_freq_appearance_value\", \"# of duplicates\"]\n",
    "data_check_duplicates_train.columns = col_list\n",
    "\n",
    "data_check_duplicates_test = pd.concat([pd.DataFrame(features),\n",
    "                                   pd.DataFrame(val_of_col_test),\n",
    "                                   pd.DataFrame(num_of_dup_test)], axis=1)\n",
    "data_check_duplicates_test.columns = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1cc194e01110b4bfe1bfe686a8e5b4088ec1178",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train data: top 10 col with most duplicate values\")\n",
    "data_check_duplicates_train.sort_values(by=\"# of duplicates\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70584e14b4cc1aba7dfd807cf6e3a4eb6c06b1d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"test data: top 10 col with most duplicate values\")\n",
    "data_check_duplicates_test.sort_values(by=\"# of duplicates\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49e2f4e8754cd282d5d3d1ca6a0a76a7318b8b09"
   },
   "source": [
    "## adding features\n",
    "- get sum, min, max, ... of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "758aaee6d614d5eb1609d85dcc0c6e434d68bfff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [data_train, data_test]:\n",
    "    df['sum'] = df[features].sum(axis=1)  \n",
    "    df['min'] = df[features].min(axis=1)\n",
    "    df['max'] = df[features].max(axis=1)\n",
    "    df['mean'] = df[features].mean(axis=1)\n",
    "    df['std'] = df[features].std(axis=1)\n",
    "    df['skew'] = df[features].skew(axis=1)\n",
    "    df['kurt'] = df[features].kurtosis(axis=1)\n",
    "    df['med'] = df[features].median(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57dc900d0e85ab7a88a56df0b4d877483bef0d2b"
   },
   "source": [
    "## Model - LGBM\n",
    "- tree based model no need scaling\n",
    "- use bayesian optimisation to optimise the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9d37fd8e4a2dc70ecf89ba4f252ce70e35c1c95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = data_train['target']\n",
    "ID = data_test[\"ID_code\"]\n",
    "features = new_features = [col for col in data_train.columns if col not in [\"ID_code\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d17b4e6c62c0aba8b8f41a51e0142b1b1fc7a0e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"# of features in the data:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b7eed62b482f659111b1058fe7b97f544d36a7e"
   },
   "source": [
    "## optimal parameters from kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ba1d52dbb4dca2a0c1f8d54141d82211f38390c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original (for cases that don't add extra features -> from kaggle kernel)\n",
    "# for model with 208 features\n",
    "# if use this for new model with extra features ->  score: 0.9\n",
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c0f36ed8d3ef177e4363d3f7795be0eec66066c",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# use x fold to check since run so slow\n",
    "folds = StratifiedKFold(n_splits=1, shuffle=False, random_state=0)\n",
    "oof = np.zeros(len(data_train))\n",
    "predictions = np.zeros(len(data_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(data_train.values, target.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(data_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(data_train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 1000000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=1000, early_stopping_rounds = 3000)\n",
    "    \n",
    "    # get 10 oof data -> cuz cv=10\n",
    "    oof[val_idx] = clf.predict(data_train.iloc[val_idx][features], \n",
    "                               num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(data_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22dfc0f9dfa3e78669c12a3b9c02ffbb83f6d76d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:150].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a19cc96f9b31eb5eb2eac6fb184887e54df880da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({\"ID_code\":data_test[\"ID_code\"].values})\n",
    "submit[\"target\"] = predictions\n",
    "submit.to_csv(\"bayesian_optimisation_lgbm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
